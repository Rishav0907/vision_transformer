{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bf64b3-5388-44ad-b2a1-f1498fed59c1",
   "metadata": {},
   "source": [
    "# Vision Transformer using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36886ec7-d2f8-4bc2-8352-2c7ab3d95cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfdafd16-acfe-4f43-9b23-9aa537337a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # self.fc1 = nn.Linear(input_dim, 512)\n",
    "        # self.fc2 = nn.Linear(512, num_classes)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.neuralNetwork=nn.Sequential(nn.Linear(input_dim,512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(512,num_classes)\n",
    "                                        )\n",
    "                                         \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.neuralNetwork(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f660b8c-d857-4eaf-baba-7f05051dd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,patch_size,output_dimension,num_of_encoder_layers,embed_dims):\n",
    "        super().__init__()\n",
    "        self.patch_size=patch_size\n",
    "        self.linear_output_dimesnion=output_dimension\n",
    "        self.num_of_encoder_layers=num_of_encoder_layers\n",
    "        self.embed_dims=embed_dims\n",
    "        self.num_heads=8\n",
    "        # patch extractor\n",
    "        self.unfold=nn.Unfold(kernel_size=self.patch_size,stride=self.patch_size)\n",
    "\n",
    "        # making each patch linear\n",
    "        self.linear=nn.Linear(self.patch_size*self.patch_size*3,output_dimension)\n",
    "        self.encoder_layer=nn.TransformerEncoderLayer( d_model=self.embed_dims, nhead=self.num_heads, dim_feedforward=2048)\n",
    "        self.transformer_encoder=nn.TransformerEncoder(self.encoder_layer,num_layers=self.num_of_encoder_layers)\n",
    "\n",
    "        self.classifier = ImageClassification(self.embed_dims, 10)\n",
    "\n",
    "    def forward(self,input_image):\n",
    "        # print(input_image)\n",
    "        batch_size,channel,height,width=input_image.shape\n",
    "        patched_image=self.unfold(input_image).permute(0,2,1)\n",
    "        # print(patched_image.shape)\n",
    "        linear_embeddings=self.linear(patched_image)\n",
    "        encoder_output=self.transformer_encoder(linear_embeddings)\n",
    "        mean_encoder_output=torch.mean(encoder_output,dim=1)\n",
    "        output = self.classifier(mean_encoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8f9b36f-a1df-4ddb-8956-3afcfe66fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class VisionTransformer(nn.Module):\n",
    "#     def __init__(self, patch_size, output_dimension, num_of_encoder_layers, embed_dims, num_heads, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.linear_output_dimension = output_dimension\n",
    "#         self.num_of_encoder_layers = num_of_encoder_layers\n",
    "#         self.embed_dims = embed_dims\n",
    "#         self.num_heads = num_heads\n",
    "        \n",
    "#         # Patch extractor\n",
    "#         self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "#         # Linear layer to map patches to embeddings\n",
    "#         self.linear = nn.Linear(self.patch_size * self.patch_size * 3, embed_dims)\n",
    "\n",
    "#         # Transformer Encoder\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dims, nhead=num_heads, dim_feedforward=2048)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_of_encoder_layers)\n",
    "\n",
    "#         # Classification head (3-layer neural network)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(embed_dims, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_image):\n",
    "#         batch_size, channels, height, width = input_image.shape\n",
    "\n",
    "#         # Extract patches\n",
    "#         patches = self.unfold(input_image).permute(0, 2, 1)  # [batch_size, num_patches, patch_dim]\n",
    "\n",
    "#         # Map patches to embeddings\n",
    "#         embeddings = self.linear(patches)\n",
    "\n",
    "#         # Rearrange embeddings to [seq_len, batch_size, embed_dims]\n",
    "#         embeddings = embeddings.view(batch_size, -1, self.embed_dims).permute(1, 0, 2)\n",
    "\n",
    "#         # Transformer Encoder\n",
    "#         encoder_output = self.transformer_encoder(embeddings)\n",
    "\n",
    "#         # Take the mean across patches\n",
    "#         mean_encoder_output = torch.mean(encoder_output, dim=0)\n",
    "\n",
    "#         # Classification\n",
    "#         output = self.classifier(mean_encoder_output)\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0fa51e2-6c3b-4168-b231-bdc9294ac8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "train_data=dataset.CIFAR10(\n",
    "    'train_data',\n",
    "    transform=transform,\n",
    "    target_transform=torchvision.transforms.Compose([\n",
    "                                 lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "                                 lambda x:F.one_hot(x,10)]),\n",
    "    # target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data=dataset.CIFAR10(\n",
    "    'test_data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=torchvision.transforms.Compose([\n",
    "                                 lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "                                 lambda x:F.one_hot(x,10)])\n",
    ")\n",
    "torch.cuda.set_device(2)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fbca9268-f166-4438-86df-98017a5cff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_dataloader=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True)\n",
    "test_dataloader=DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ad32c9ce-11ac-4a34-b709-d866973ceeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample=next(iter(train_dataloader))\n",
    "# image=sample[0][0]\n",
    "# label=sample[1][0]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(np.transpose(image,(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29f2df41-5cac-46e2-b08d-c92f42ae8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f84a34b6-49b5-486e-a45c-d6abfacbd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c78a6ff-b914-459d-ba83-300c84d2f2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x10 and 256x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 59\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# def test():\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#     transformer.eval()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#     correct = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     accuracy = correct / total\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#     print(f\"Test Accuracy: {accuracy:.4f}\")\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(nn.Embedding(1,res))\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# classifier=ImageClassification(\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# res= res.view(BATCH_SIZE, 3, 4, 4, -1).permute(0, 4, 1, 2, 3)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[104], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(input_data)\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mimage_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[70], line 13\u001b[0m, in \u001b[0;36mImageClassification.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x10 and 256x512)"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE=0.001\n",
    "EPOCHS=10\n",
    "\n",
    "import torch.optim as optim\n",
    "transformer=VisionTransformer(patch_size=4,output_dimension=256,num_of_encoder_layers=4,embed_dims=256)\n",
    "# ,num_heads=8,num_classes=10)\n",
    "transformer=transformer.to(device)\n",
    "image_classifier = ImageClassification(input_dim=256, num_classes=10)\n",
    "image_classifier.to(device)\n",
    "cross_entropy_loss=nn.CrossEntropyLoss()\n",
    "optimizer=optimizer = optim.Adam(list(transformer.parameters()) + list(image_classifier.parameters()), lr=LEARNING_RATE)\n",
    "# res=transformer.createImagePatches(sample[0])\n",
    "\n",
    "# def train():\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         transformer.train()\n",
    "#         loss=0\n",
    "#         for input_data,labels in train_dataloader:\n",
    "#             input_data,labels=input_data.to(device),labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             output=transformer(input_data)\n",
    "#             loss=cross_entropy_loss(output,labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             loss += loss.item() * input_data.size(0)\n",
    "#         epoch_loss = loss / len(train_dataloader.dataset)\n",
    "#         print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "def train():\n",
    "    for epoch in range(EPOCHS):\n",
    "        transformer.train()\n",
    "        image_classifier.train()\n",
    "        epoch_loss = 0\n",
    "        for input_data, labels in train_dataloader:\n",
    "            input_data, labels = input_data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = transformer(input_data)\n",
    "            output = image_classifier(output)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * input_data.size(0)\n",
    "        epoch_loss /= len(train_dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
    "# def test():\n",
    "#     transformer.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_dataloader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "#             outputs = transformer(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "train()\n",
    "# print(nn.Embedding(1,res))\n",
    "# classifier=ImageClassification(\n",
    "# res= res.view(BATCH_SIZE, 3, 4, 4, -1).permute(0, 4, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61283b1c-ef7a-48dd-bbea-09df22e947d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    transformer.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = transformer(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += torch.eq(predicted, labels).sum().item()  # Accumulate correct predictions\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "test()  # Call the test function to evaluate the model after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a1ec698-0834-47ef-8791-80bf4eb7b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f453787e610>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGiCAYAAAAba+fDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgd0lEQVR4nO3df2yV5f3/8dcB+oON9ijUljJKKZtWfqjDVm2Z4JCkUCIBJftgNLXsh1sXfgQ6Mim6TLcs3T5hjhkVxgSMopNkBYeBMZpIWw1FKSviRulYVmnHpxXL4BSKngK7vn/45cTaQ6F47tP23ecjOQnn7nWfXpeX2Kd37576nHNOAAAARgzq7QkAAABEEnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABM8TRuTp06pYKCAvn9fvn9fhUUFOj06dPdnrNw4UL5fL5Oj5ycHC+nCQAADBni5Ys/9NBD+ve//61du3ZJkr7//e+roKBAb7zxRrfnzZo1S5s2bQo9j42N9XKaAADAEM/ipq6uTrt27dK+fft01113SZJ+//vfKzc3V/X19crMzLzsuXFxcRo5cqRXUwMAAIZ5FjfV1dXy+/2hsJGknJwc+f1+7d27t9u4qaioUHJysq677jrdc889+sUvfqHk5OSwY4PBoILBYOj5f//7X/3nP//RiBEj5PP5IrcgAAAQUc45nTlzRqNGjdKgQZG7U8azuGlpaQkbJMnJyWppabnsefn5+frWt76l9PR0NTQ06Cc/+YnuvfdeHThwQHFxcV3Gl5aW6qmnnoro3AEAQPQ0NTVp9OjREXu9HsfNk08+ecWY2L9/vySFvXLinOv2isqCBQtCf540aZKys7OVnp6uHTt26IEHHugyvqSkRMXFxaHngUBAY8aM0Tvv7tOwYcOuuB70b7ExXYMXNn38cXtvTwFR9H/NJ3p7CoiC9vZ2zX/gfiUkJET0dXscN4sXL9aDDz7Y7ZixY8fq0KFD+vDDD7t87KOPPlJKSspVf77U1FSlp6fr6NGjYT8eFxcX9orOsGHDIv4PC30PcTNwDBnCO1cMJF9uI2YHkkjfRtLjuElKSlJSUtIVx+Xm5ioQCOjdd9/VnXfeKUl65513FAgENGXKlKv+fCdPnlRTU5NSU1N7OlUAADAAefa/QuPHj9esWbP06KOPat++fdq3b58effRR3XfffZ1uJr755pu1bds2SdLZs2e1YsUKVVdX64MPPlBFRYXmzJmjpKQk3X///V5NFQAAGOLpdd5XXnlFt9xyi/Ly8pSXl6dbb71VL7/8cqcx9fX1CgQCkqTBgwfr/fff19y5c3XTTTepsLBQN910k6qrq/kWEwAAuCqevonf8OHDtXnz5m7HOOdCfx46dKj+8pe/eDklAABgHHfoAQAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMiUrcPP/888rIyFB8fLyysrL01ltvdTu+srJSWVlZio+P17hx47Ru3bpoTBMAABjgedxs2bJFy5Yt0+OPP67a2lpNnTpV+fn5amxsDDu+oaFBs2fP1tSpU1VbW6tVq1Zp6dKlKisr83qqAADAAJ9zznn5Ce666y7dfvvtWrt2bejY+PHjNW/ePJWWlnYZ/9hjj2n79u2qq6sLHSsqKtJ7772n6urqLuODwaCCwWDoeVtbm9LS0vT3w39TQkJChFeDviY2Jq63p4Ao+fjj9t6eAqLo38dbensKiIL29nbNmpmnQCCgxMTEiL2up1duOjo6dODAAeXl5XU6npeXp71794Y9p7q6usv4mTNnqqamRufPn+8yvrS0VH6/P/RIS0uL3AIAAEC/42nctLa26uLFi0pJSel0PCUlRS0t4au8paUl7PgLFy6otbW1y/iSkhIFAoHQo6mpKXILAAAA/c6QaHwSn8/X6blzrsuxK40Pd1yS4uLiFBfHtyYAAMCnPL1yk5SUpMGDB3e5SnPixIkuV2cuGTlyZNjxQ4YM0YgRIzybKwAAsMHTuImNjVVWVpbKy8s7HS8vL9eUKVPCnpObm9tl/O7du5Wdna2YmBjP5goAAGzw/EfBi4uL9cILL2jjxo2qq6vT8uXL1djYqKKiIkmf3jPzyCOPhMYXFRXp2LFjKi4uVl1dnTZu3KgNGzZoxYoVXk8VAAAY4Pk9NwsWLNDJkyf1s5/9TM3NzZo0aZJ27typ9PR0SVJzc3On97zJyMjQzp07tXz5cj333HMaNWqUnnnmGc2fP9/rqQIAAAM8f5+baGtra5Pf7+d9bgYI3udm4OB9bgYW3udmYOiX73MDAAAQbcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwJSpx8/zzzysjI0Px8fHKysrSW2+9ddmxFRUV8vl8XR5HjhyJxlQBAEA/53ncbNmyRcuWLdPjjz+u2tpaTZ06Vfn5+WpsbOz2vPr6ejU3N4ceN954o9dTBQAABngeN08//bS++93v6nvf+57Gjx+vNWvWKC0tTWvXru32vOTkZI0cOTL0GDx4sNdTBQAABgzx8sU7Ojp04MABrVy5stPxvLw87d27t9tzJ0+erE8++UQTJkzQE088oenTp4cdFwwGFQwGQ8/b2tokSYN8gzTIxy1F1v33wsXengKi5Exbe29PAVHUcvpcb08BUXDunDf77OlX/9bWVl28eFEpKSmdjqekpKilpSXsOampqVq/fr3Kysq0detWZWZmasaMGaqqqgo7vrS0VH6/P/RIS0uL+DoAAED/4emVm0t8Pl+n5865LscuyczMVGZmZuh5bm6umpqatHr1ak2bNq3L+JKSEhUXF4eet7W1ETgAAAxgnl65SUpK0uDBg7tcpTlx4kSXqzndycnJ0dGjR8N+LC4uTomJiZ0eAABg4PI0bmJjY5WVlaXy8vJOx8vLyzVlypSrfp3a2lqlpqZGenoAAMAgz78tVVxcrIKCAmVnZys3N1fr169XY2OjioqKJH36baXjx4/rpZdekiStWbNGY8eO1cSJE9XR0aHNmzerrKxMZWVlXk8VAAAY4HncLFiwQCdPntTPfvYzNTc3a9KkSdq5c6fS09MlSc3NzZ3e86ajo0MrVqzQ8ePHNXToUE2cOFE7duzQ7NmzvZ4qAAAwwOecc709iUhqa2uT3+9XXd1hJSQk9PZ04LFB/AaRAaP15MnengKiqL7pw96eAqLg3Ll2Ff7PPAUCgYjeM8tXBgAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKZ7GTVVVlebMmaNRo0bJ5/Pp9ddfv+I5lZWVysrKUnx8vMaNG6d169Z5OUUAAGCMp3HT3t6u2267Tc8+++xVjW9oaNDs2bM1depU1dbWatWqVVq6dKnKysq8nCYAADBkiJcvnp+fr/z8/Ksev27dOo0ZM0Zr1qyRJI0fP141NTVavXq15s+fH/acYDCoYDAYet7W1vaF5gwAAPq3PnXPTXV1tfLy8jodmzlzpmpqanT+/Pmw55SWlsrv94ceaWlp0ZgqAADoo/pU3LS0tCglJaXTsZSUFF24cEGtra1hzykpKVEgEAg9mpqaojFVAADQR3n6balr4fP5Oj13zoU9fklcXJzi4uI8nxcAAOgf+tSVm5EjR6qlpaXTsRMnTmjIkCEaMWJEL80KAAD0J30qbnJzc1VeXt7p2O7du5Wdna2YmJhemhUAAOhPPI2bs2fP6uDBgzp48KCkT3/U++DBg2psbJT06f0yjzzySGh8UVGRjh07puLiYtXV1Wnjxo3asGGDVqxY4eU0AQCAIZ7ec1NTU6Pp06eHnhcXF0uSCgsL9eKLL6q5uTkUOpKUkZGhnTt3avny5Xruuec0atQoPfPMM5f9MXAAAIDP87lLd+wa0dbWJr/fr7q6w0pISOjt6cBjg/rWd1bhodaTJ3t7Coii+qYPe3sKiIJz59pV+D/zFAgElJiYGLHX5SsDAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmeBo3VVVVmjNnjkaNGiWfz6fXX3+92/EVFRXy+XxdHkeOHPFymgAAwJAhXr54e3u7brvtNn3729/W/Pnzr/q8+vp6JSYmhp7fcMMNXkwPAAAY5Gnc5OfnKz8/v8fnJScn67rrrov8hAAAgHmexs21mjx5sj755BNNmDBBTzzxhKZPn37ZscFgUMFgMPS8ra1NkjTsy8OUMCzB87mid8XExPT2FBAlQ780tLengChy8vX2FBAFZ8+e9eR1+9QNxampqVq/fr3Kysq0detWZWZmasaMGaqqqrrsOaWlpfL7/aFHWlpaFGcMAAD6Gp9zzkXlE/l82rZtm+bNm9ej8+bMmSOfz6ft27eH/Xi4KzdpaWlqamzsdN8ObOLKzcDx2b/nsK+x6XhvTwFRcPbsWX3jG7kKBAIR/Zrdp67chJOTk6OjR49e9uNxcXFKTEzs9AAAAANXn4+b2tpapaam9vY0AABAP+HpDcVnz57VP//5z9DzhoYGHTx4UMOHD9eYMWNUUlKi48eP66WXXpIkrVmzRmPHjtXEiRPV0dGhzZs3q6ysTGVlZV5OEwAAGOJp3NTU1HT6Safi4mJJUmFhoV588UU1NzersbEx9PGOjg6tWLFCx48f19ChQzVx4kTt2LFDs2fP9nKaAADAkKjdUBwtbW1t8vv93FA8QHBD8cDBDcUDCzcUDwwD9oZiAACAniBuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwhbgBAACmEDcAAMAU4gYAAJhC3AAAAFOIGwAAYApxAwAATCFuAACAKcQNAAAwxdO4KS0t1R133KGEhAQlJydr3rx5qq+vv+J5lZWVysrKUnx8vMaNG6d169Z5OU0AAGCIp3FTWVmpRYsWad++fSovL9eFCxeUl5en9vb2y57T0NCg2bNna+rUqaqtrdWqVau0dOlSlZWVeTlVAABghM8556L1yT766CMlJyersrJS06ZNCzvmscce0/bt21VXVxc6VlRUpPfee0/V1dVX/BxtbW3y+/1qamxUYmJixOaOvikmJqa3p4AoCQaDvT0FRFFj0/HengKi4OzZs/rGN3IVCAQi+jU7qvfcBAIBSdLw4cMvO6a6ulp5eXmdjs2cOVM1NTU6f/58l/HBYFBtbW2dHgAAYOCKWtw451RcXKy7775bkyZNuuy4lpYWpaSkdDqWkpKiCxcuqLW1tcv40tJS+f3+0CMtLS3icwcAAP1H1OJm8eLFOnTokP7whz9ccazP5+v0/NJ3zj5/XJJKSkoUCARCj6ampshMGAAA9EtDovFJlixZou3bt6uqqkqjR4/uduzIkSPV0tLS6diJEyc0ZMgQjRgxosv4uLg4xcXFRXS+AACg//L0yo1zTosXL9bWrVv15ptvKiMj44rn5Obmqry8vNOx3bt3Kzs7m5tHAQDAFXkaN4sWLdLmzZv16quvKiEhQS0tLWppadHHH38cGlNSUqJHHnkk9LyoqEjHjh1TcXGx6urqtHHjRm3YsEErVqzwcqoAAMAIT+Nm7dq1CgQC+uY3v6nU1NTQY8uWLaExzc3NamxsDD3PyMjQzp07VVFRoa9//ev6+c9/rmeeeUbz58/3cqoAAMAIT++5uZq30HnxxRe7HLvnnnv017/+1YMZAQAA6/jdUgAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAUzyNm9LSUt1xxx1KSEhQcnKy5s2bp/r6+m7PqaiokM/n6/I4cuSIl1MFAABGeBo3lZWVWrRokfbt26fy8nJduHBBeXl5am9vv+K59fX1am5uDj1uvPFGL6cKAACMGOLli+/atavT802bNik5OVkHDhzQtGnTuj03OTlZ11133RU/RzAYVDAYDD0PBAKSpDNnzvR8wuh3YmJiensKiJLP/j2HfWfPnu3tKSAKLl3scM5F9HU9jZvPuxQew4cPv+LYyZMn65NPPtGECRP0xBNPaPr06WHHlZaW6qmnnupyfMLEiV9ssgAAICpOnjwpv98fsdfzuUjn0mU45zR37lydOnVKb7311mXH1dfXq6qqSllZWQoGg3r55Ze1bt06VVRUhL3a8/krN6dPn1Z6eroaGxsj+g+qr2tra1NaWpqampqUmJjY29OJmoG47oG4Zmlgrnsgrlli3QNp3YFAQGPGjNGpU6eu6rs1VytqV24WL16sQ4cO6e233+52XGZmpjIzM0PPc3Nz1dTUpNWrV4eNm7i4OMXFxXU57vf7B8y/HJ+VmJjIugeIgbhmaWCueyCuWWLdA8mgQZG9BTgqPwq+ZMkSbd++XXv27NHo0aN7fH5OTo6OHj3qwcwAAIA1nl65cc5pyZIl2rZtmyoqKpSRkXFNr1NbW6vU1NQIzw4AAFjkadwsWrRIr776qv70pz8pISFBLS0tkj79ltHQoUMlSSUlJTp+/LheeuklSdKaNWs0duxYTZw4UR0dHdq8ebPKyspUVlZ2VZ8zLi5OP/3pT8N+q8oy1j1w1j0Q1ywNzHUPxDVLrHsgrdurNXt6Q7HP5wt7fNOmTVq4cKEkaeHChfrggw9UUVEhSfrf//1frV+/XsePH9fQoUM1ceJElZSUaPbs2V5NEwAAGBK1n5YCAACIBn63FAAAMIW4AQAAphA3AADAFOIGAACYYiJuTp06pYKCAvn9fvn9fhUUFOj06dPdnrNw4UL5fL5Oj5ycnOhM+Bo9//zzysjIUHx8vLKysrr9NRbSp7+VPSsrS/Hx8Ro3bpzWrVsXpZlGTk/WXFFR0WVPfT6fjhw5EsUZf3FVVVWaM2eORo0aJZ/Pp9dff/2K5/T3ve7pmi3sdWlpqe644w4lJCQoOTlZ8+bNU319/RXP6+97fS3rtrDfa9eu1a233hp69+Hc3Fz9+c9/7vac/r7XPV1zJPfZRNw89NBDOnjwoHbt2qVdu3bp4MGDKigouOJ5s2bNUnNzc+ixc+fOKMz22mzZskXLli3T448/rtraWk2dOlX5+flqbGwMO76hoUGzZ8/W1KlTVVtbq1WrVmnp0qVX/X5BfUFP13xJfX19p3298cYbozTjyGhvb9dtt92mZ5999qrGW9jrnq75kv6815WVlVq0aJH27dun8vJyXbhwQXl5eaHfkhyOhb2+lnVf0p/3e/To0frlL3+pmpoa1dTU6N5779XcuXP197//Pex4C3vd0zVfEpF9dv3c4cOHnSS3b9++0LHq6monyR05cuSy5xUWFrq5c+dGYYaRceedd7qioqJOx26++Wa3cuXKsON//OMfu5tvvrnTsR/84AcuJyfHszlGWk/XvGfPHifJnTp1Kgqziw5Jbtu2bd2OsbDXn3U1a7a41ydOnHCSXGVl5WXHWNtr565u3Rb32znnrr/+evfCCy+E/ZjFvXau+zVHcp/7/ZWb6upq+f1+3XXXXaFjOTk58vv92rt3b7fnVlRUKDk5WTfddJMeffRRnThxwuvpXpOOjg4dOHBAeXl5nY7n5eVddo3V1dVdxs+cOVM1NTU6f/68Z3ONlGtZ8yWTJ09WamqqZsyYoT179ng5zT6hv+/1F2FprwOBgCRp+PDhlx1jca+vZt2XWNnvixcv6rXXXlN7e7tyc3PDjrG211ez5ksisc/9Pm5aWlqUnJzc5XhycnLo1z2Ek5+fr1deeUVvvvmmfv3rX2v//v269957FQwGvZzuNWltbdXFixeVkpLS6XhKSspl19jS0hJ2/IULF9Ta2urZXCPlWtacmpqq9evXq6ysTFu3blVmZqZmzJihqqqqaEy51/T3vb4W1vbaOafi4mLdfffdmjRp0mXHWdvrq123lf1+//33NWzYMMXFxamoqEjbtm3ThAkTwo61stc9WXMk99nT3y31RTz55JN66qmnuh2zf/9+SeF/zYNz7rK//kGSFixYEPrzpEmTlJ2drfT0dO3YsUMPPPDANc7aW59fz5XWGG58uON9WU/WnJmZqczMzNDz3NxcNTU1afXq1Zo2bZqn8+xtFva6J6zt9eLFi3Xo0CG9/fbbVxxraa+vdt1W9jszM1MHDx7U6dOnVVZWpsLCQlVWVl72i72Fve7JmiO5z302bhYvXqwHH3yw2zFjx47VoUOH9OGHH3b52EcffdSleruTmpqq9PR0HT16tMdz9VpSUpIGDx7c5YrFiRMnLrvGkSNHhh0/ZMgQjRgxwrO5Rsq1rDmcnJwcbd68OdLT61P6+15HSn/d6yVLlmj79u2qqqrS6NGjux1raa97su5w+uN+x8bG6mtf+5okKTs7W/v379dvf/tb/e53v+sy1spe92TN4VzrPvfZuElKSlJSUtIVx+Xm5ioQCOjdd9/VnXfeKUl65513FAgENGXKlKv+fCdPnlRTU5NSU1Ovec5eiY2NVVZWlsrLy3X//feHjpeXl2vu3Llhz8nNzdUbb7zR6dju3buVnZ2tmJgYT+cbCdey5nBqa2v75J5GUn/f60jpb3vtnNOSJUu0bds2VVRUKCMj44rnWNjra1l3OP1tv8Nxzl32VggLex1Od2sO55r3+QvfktwHzJo1y916662uurraVVdXu1tuucXdd999ncZkZma6rVu3OuecO3PmjPvRj37k9u7d6xoaGtyePXtcbm6u+8pXvuLa2tp6YwlX9Nprr7mYmBi3YcMGd/jwYbds2TL35S9/2X3wwQfOOedWrlzpCgoKQuP/9a9/uS996Utu+fLl7vDhw27Dhg0uJibG/fGPf+ytJfRYT9f8m9/8xm3bts394x//cH/729/cypUrnSRXVlbWW0u4JmfOnHG1tbWutrbWSXJPP/20q62tdceOHXPO2dzrnq7Zwl7/8Ic/dH6/31VUVLjm5ubQ49y5c6ExFvf6WtZtYb9LSkpcVVWVa2hocIcOHXKrVq1ygwYNcrt373bO2dzrnq45kvtsIm5OnjzpHn74YZeQkOASEhLcww8/3OVHySS5TZs2OeecO3funMvLy3M33HCDi4mJcWPGjHGFhYWusbEx+pPvgeeee86lp6e72NhYd/vtt3f60cnCwkJ3zz33dBpfUVHhJk+e7GJjY93YsWPd2rVrozzjL64na/7Vr37lvvrVr7r4+Hh3/fXXu7vvvtvt2LGjF2b9xVz6ccjPPwoLC51zNve6p2u2sNfh1vvZ/045Z3Ovr2XdFvb7O9/5Tui/ZTfccIObMWNG6Iu8czb3uqdrjuQ++5z7/3coAQAAGNDvfxQcAADgs4gbAABgCnEDAABMIW4AAIApxA0AADCFuAEAAKYQNwAAwBTiBgAAmELcAAAAU4gbAABgCnEDAABM+X++kVKNhzHgcQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new=VisionTransformer(patch_size=4,output_dimension=256)\n",
    "res=new.createImagePatches(image)\n",
    "# res[0][0]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(res[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75dfb65e-0123-41ba-87df-4e51d0641a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.8521\n",
      "Epoch [2/10], Loss: 1.6783\n",
      "Epoch [3/10], Loss: 1.5985\n",
      "Epoch [4/10], Loss: 1.5593\n",
      "Epoch [5/10], Loss: 1.5303\n",
      "Epoch [6/10], Loss: 1.5018\n",
      "Epoch [7/10], Loss: 1.4766\n",
      "Epoch [8/10], Loss: 1.4622\n",
      "Epoch [9/10], Loss: 1.4408\n",
      "Epoch [10/10], Loss: 1.4289\n",
      "Test Accuracy: 0.4775\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "input_size = 32 * 32 * 3  # CIFAR-10 image size is 32x32x3\n",
    "hidden_size = 512\n",
    "num_classes = 10\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.view(images.size(0), -1)  # Flatten the image tensor\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.view(images.size(0), -1)  # Flatten the image tensor\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d586cd5-62f2-4c33-b3af-6a38d50c331a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
