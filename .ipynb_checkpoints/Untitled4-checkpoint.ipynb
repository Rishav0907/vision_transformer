{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab6249f-8f04-46a6-b537-61c165fdc4cc",
   "metadata": {},
   "source": [
    "# Vision Transformer using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c576d554-58ff-41d9-9e49-95b550584374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from torch.utils.data import random_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d883ce98-cf92-48dd-8942-9d3cb832d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, f\"Can't divide dimension {embed_dim} into {num_heads} heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = embed_dim // num_heads\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # sequences: (seq_length, N, embed_dim)\n",
    "        sequences = sequences.permute(1, 0, 2)  # (N, seq_length, embed_dim)\n",
    "        attn_output, attn_weights = self.multihead_attn(sequences, sequences, sequences)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # (seq_length, N, embed_dim)\n",
    "        return attn_output,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d841d13e-fc03-49dd-b53e-8dbac834bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,num_heads,hidden_dimension,mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        self.normalize_layer_1=nn.LayerNorm(hidden_dimension)\n",
    "        self.multi_headed_self_attention=MultiHeadedSelfAttention(num_heads=num_heads,embed_dim=hidden_dimension)\n",
    "        self.normalize_layer_2=nn.LayerNorm(hidden_dimension)\n",
    "        self.neural_network=nn.Sequential(\n",
    "            nn.Linear(hidden_dimension,mlp_ratio*hidden_dimension),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio*hidden_dimension,hidden_dimension)\n",
    "        )\n",
    "    def forward(self,encoder_input):\n",
    "        norm_layer1_output=self.normalize_layer_1(encoder_input)\n",
    "        self_attention_output,attention_weights=self.multi_headed_self_attention(norm_layer1_output)\n",
    "        norm_layer2_output=self.normalize_layer_2(self_attention_output)\n",
    "        neural_network_output=self.neural_network(norm_layer2_output)\n",
    "        final_output=neural_network_output\n",
    "        return final_output,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "416933a6-417a-41d1-9584-2d935b25ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,patch_size,image_size,hidden_dimension,num_heads,out_dimension,num_encoder_blocks=2):\n",
    "        super().__init__()\n",
    "        self.patch_size=patch_size\n",
    "        self.hidden_dimension=hidden_dimension\n",
    "        self.num_heads=num_heads\n",
    "        self.num_encoder_blocks=num_encoder_blocks\n",
    "        self.output_dimension=out_dimension\n",
    "        # dimension of the input patches after they have been flattened\n",
    "        self.input_dimension = int(3 * self.patch_size * self.patch_size)\n",
    "        # linear embedding\n",
    "        self.linear_mapper = nn.Linear(self.input_dimension, self.hidden_dimension)\n",
    "        #class token \n",
    "        self.class_token=nn.Parameter(torch.randn(1,self.hidden_dimension))\n",
    "        batch_size,channel,height,width=image_size\n",
    "        num_of_patches=(height*width)/(self.patch_size**2)\n",
    "        # print(num_of_patches)\n",
    "        self.positional_encodings = self._generate_positional_encodings(int(num_of_patches), self.hidden_dimension)\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(hidden_dimension=self.hidden_dimension, num_heads=self.num_heads) for _ in range(self.num_encoder_blocks)])\n",
    "        \n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dimension, self.output_dimension),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def _generate_positional_encodings(self, num_patches, hidden_dimension):\n",
    "        position_encodings = torch.zeros(num_patches + 1, hidden_dimension)  # +1 for class token\n",
    "        position_encodings[1:, 0::2] = torch.sin(self._get_angles(torch.arange(0, num_patches).float(),\n",
    "                                                                                     2 * torch.arange(0, hidden_dimension // 2).float() / hidden_dimension))\n",
    "        position_encodings[1:, 1::2] = torch.cos(self._get_angles(torch.arange(0, num_patches).float(),\n",
    "                                                                                     2 * torch.arange(0, hidden_dimension // 2).float() / hidden_dimension))\n",
    "        return position_encodings.unsqueeze(0)\n",
    "\n",
    "    def _get_angles(self, positions, i):\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(self.hidden_dimension).float())\n",
    "        return positions.unsqueeze(-1) * angle_rates\n",
    "   \n",
    "    def forward(self,input_images):\n",
    "        # image_patches=rearrange(input_images, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=self.patch_size, s2=self.patch_size)\n",
    "        # nn.functional.unfold(input_images,kernel_size=self.patch_size,stride=self.patch_size)\n",
    "        batch_size=input_images.shape[0]\n",
    "        # unfold = nn.Unfold(self.patch_size,stride=1)\n",
    "        image_patches=input_images.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape the patches\n",
    "        image_patches = image_patches.contiguous().view(input_images.size(0), -1, self.patch_size * self.patch_size * input_images.size(1))\n",
    "        tokens=self.linear_mapper(image_patches)\n",
    "\n",
    "        # Adding the class_tokens to the tokens\n",
    "        tokens_with_class=torch.stack([torch.vstack((self.class_token,tokens[i])) for i in range(len(tokens))])\n",
    "        # class_token = self.class_token.expand(batch_size, -1)\n",
    "        # tokens_with_class = torch.cat((class_token.unsqueeze(1), tokens), dim=1)\n",
    "        tokens_with_position = tokens_with_class + self.positional_encodings[:, :tokens_with_class.size(1)]\n",
    "\n",
    "        # Add positional encodings\n",
    "        # tokens_with_position = tokens_with_class + self.positional_encodings[:, :tokens_with_class.size(1)]\n",
    "        # print(tokens_with_position)\n",
    "        attention_weights=[]\n",
    "        for block in self.blocks:\n",
    "            tokens_with_position ,attn_weight= block(tokens_with_position)\n",
    "            attention_weights.append(attn_weight)\n",
    "         # Getting the classification token only\n",
    "        tokens_with_position = tokens_with_position[:, 0]\n",
    "        \n",
    "        return self.mlp(tokens_with_position),attention_weights\n",
    "        # return image_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3245218d-d525-42ce-988f-c515a42375ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    'train_data',\n",
    "    transform=transform,\n",
    "    target_transform=transforms.Compose([\n",
    "        lambda x: torch.tensor(x),  # Convert label to tensor\n",
    "    ]),\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    'test_data',\n",
    "    transform=transform,\n",
    "    target_transform=transforms.Compose([\n",
    "        lambda x: torch.tensor(x),  # Convert label to tensor\n",
    "    ]),\n",
    "    train=False,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55c13538-9ff0-4a25-afb2-1735b3fa6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  None (NVIDIA GeForce RTX 3090)\n"
     ]
    }
   ],
   "source": [
    "# Defining model and training options\n",
    "device = torch.cuda.set_device(1)\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "transformer_model = VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=2, out_dimension=10,patch_size=3)\n",
    "\n",
    "# model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "N_EPOCHS = 5\n",
    "LR = 0.005\n",
    "\n",
    "# # Training loop\n",
    "# def train():\n",
    "#     optimizer = Adam(model.parameters(), lr=LR)\n",
    "#     criterion = CrossEntropyLoss()\n",
    "#     for epoch in range(N_EPOCHS):\n",
    "#         train_loss = 0.0\n",
    "#         for batch in train_loader:\n",
    "#             x, y = batch\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "#             y_hat = model(x)\n",
    "#             loss = criterion(y_hat, y)\n",
    "    \n",
    "#             train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "    \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#         print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "criterion = CrossEntropyLoss()\n",
    "# Test loop\n",
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "    \n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dce108-eb31-47ca-8c19-34b832e36508",
   "metadata": {},
   "source": [
    "# Training the model with different data sizes: 5% , 10% , 25% , 50% , 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6bab2e9-cdfa-4ab7-b19c-a3cad36f6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "LR = 0.005\n",
    "def train(train_data_loader,model):\n",
    "    train_loader=train_data_loader\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "    \n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d295c53-e6a6-4bce-94b1-bc6a8cb5be8a",
   "metadata": {},
   "source": [
    "## Trainig with 5% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6dc7a07-cb3f-4671-827d-72528487764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.30\n",
      "Epoch 2/5 loss: 2.29\n",
      "Epoch 3/5 loss: 2.27\n",
      "Epoch 4/5 loss: 2.27\n",
      "Epoch 5/5 loss: 2.26\n"
     ]
    }
   ],
   "source": [
    "five_percent = int(0.05 * len(train_data))\n",
    "\n",
    "subset_dataset, _ = random_split(train_data, [five_percent, len(train_data) - five_percent])\n",
    "\n",
    "# Create a DataLoader for the subset dataset\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(subset_loader,model=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f140c81-a642-4ddd-bf5c-dc4653fef868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.26\n",
      "Test accuracy: 17.07%\n"
     ]
    }
   ],
   "source": [
    "test(model=transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84415da-5b94-4e1b-b7a6-cc4d3efd469a",
   "metadata": {},
   "source": [
    "## Training with 10% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79c3b13d-14d5-41cd-8afb-af637431967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.27\n",
      "Epoch 2/5 loss: 2.26\n",
      "Epoch 3/5 loss: 2.26\n",
      "Epoch 4/5 loss: 2.26\n",
      "Epoch 5/5 loss: 2.25\n"
     ]
    }
   ],
   "source": [
    "ten_percent = int(0.1 * len(train_data))\n",
    "\n",
    "subset_dataset, _ = random_split(train_data, [ten_percent, len(train_data) - ten_percent])\n",
    "\n",
    "# Create a DataLoader for the subset dataset\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(subset_loader,model=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bcf447f-fe02-4391-8174-ad50562a236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.25\n",
      "Test accuracy: 18.99%\n"
     ]
    }
   ],
   "source": [
    "test(model=transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087703f-6ceb-477e-905f-8b06b9299fc4",
   "metadata": {},
   "source": [
    "## Training with 25% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26e70dfc-8f1c-4139-a85e-16d4141cda4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.25\n",
      "Epoch 2/5 loss: 2.23\n",
      "Epoch 3/5 loss: 2.22\n",
      "Epoch 4/5 loss: 2.22\n",
      "Epoch 5/5 loss: 2.22\n"
     ]
    }
   ],
   "source": [
    "twenty_five_percent = int(0.25 * len(train_data))\n",
    "\n",
    "subset_dataset, _ = random_split(train_data, [twenty_five_percent, len(train_data) - twenty_five_percent])\n",
    "\n",
    "# Create a DataLoader for the subset dataset\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(subset_loader,model=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef1caa6-99bc-46bf-b052-c68b5d5d89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.21\n",
      "Test accuracy: 23.17%\n"
     ]
    }
   ],
   "source": [
    "test(model=transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338b016-a7bf-4e76-ad16-2f8588d21101",
   "metadata": {},
   "source": [
    "## Training with 50% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9870b574-cec4-465a-8970-74332e80ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.21\n",
      "Epoch 2/5 loss: 2.20\n",
      "Epoch 3/5 loss: 2.19\n",
      "Epoch 4/5 loss: 2.18\n",
      "Epoch 5/5 loss: 2.18\n"
     ]
    }
   ],
   "source": [
    "fifty_percent = int(0.5 * len(train_data))\n",
    "\n",
    "subset_dataset, _ = random_split(train_data, [fifty_percent, len(train_data) - fifty_percent])\n",
    "\n",
    "# Create a DataLoader for the subset dataset\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(subset_loader,model=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54df80d2-474a-4216-af9e-c8ca58488716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.18\n",
      "Test accuracy: 26.29%\n"
     ]
    }
   ],
   "source": [
    "test(model=transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebc9ad-10d3-46ae-b8be-8040b185d872",
   "metadata": {},
   "source": [
    "## Training with 100% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17fe1979-a6fa-4bac-8a7c-96da671565d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.18\n",
      "Epoch 2/5 loss: 2.17\n",
      "Epoch 3/5 loss: 2.17\n",
      "Epoch 4/5 loss: 2.16\n",
      "Epoch 5/5 loss: 2.16\n"
     ]
    }
   ],
   "source": [
    "subset_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train(subset_loader,model=transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2756ef4f-2d88-4634-a301-55b0d02a0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.15\n",
      "Test accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "test(model=transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aba105-58eb-45f6-a9f3-dfe600867076",
   "metadata": {},
   "source": [
    "# Training with different patch sizes: (4x4) , (8x8) , (16x16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecdbbf-cddf-4dda-8fd4-ef6e4e73af1b",
   "metadata": {},
   "source": [
    "## Training with patch size (4x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8b0fd5-952c-4fba-92e4-fda6876bf61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.24\n",
      "Epoch 2/5 loss: 2.21\n",
      "Epoch 3/5 loss: 2.19\n",
      "Epoch 4/5 loss: 2.18\n",
      "Epoch 5/5 loss: 2.19\n"
     ]
    }
   ],
   "source": [
    "model_patch_4=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=2, out_dimension=10,patch_size=4)\n",
    "train_data_loader=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "train(train_data_loader,model=model_patch_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fdd67c0-da04-4067-8794-53128a04c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.16\n",
      "Test accuracy: 28.66%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_patch_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54722128-fd39-46cc-ba29-32e4406f56bb",
   "metadata": {},
   "source": [
    "## Training with patch size (8x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fedf35ab-ce33-4446-8d61-033bc408ec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.26\n",
      "Epoch 2/5 loss: 2.24\n",
      "Epoch 3/5 loss: 2.23\n",
      "Epoch 4/5 loss: 2.22\n",
      "Epoch 5/5 loss: 2.22\n"
     ]
    }
   ],
   "source": [
    "model_patch_8=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=2, out_dimension=10,patch_size=8)\n",
    "train(train_data_loader,model=model_patch_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4ffa7bc-f9f0-4978-9b15-82fac9ab78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.20\n",
      "Test accuracy: 23.97%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_patch_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9639788-5fe3-4ca3-8819-e801d8ab6749",
   "metadata": {},
   "source": [
    "## Training with patch size (16x16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cdfc424-1e92-4395-9ab8-8319d6f2a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.25\n",
      "Epoch 2/5 loss: 2.23\n",
      "Epoch 3/5 loss: 2.22\n",
      "Epoch 4/5 loss: 2.22\n",
      "Epoch 5/5 loss: 2.20\n"
     ]
    }
   ],
   "source": [
    "model_patch_16=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=2, out_dimension=10,patch_size=16)\n",
    "train(train_data_loader,model=model_patch_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b655fd6-4006-409b-8776-6ab07cc8ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.19\n",
      "Test accuracy: 25.48%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_patch_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f07654-4410-4f83-982d-2dda886e459d",
   "metadata": {},
   "source": [
    "# Training with different number of attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067eb46-89c1-438f-9e85-519d0bb660e7",
   "metadata": {},
   "source": [
    "## Training with 4 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "803c0b4b-d3a9-49ac-a977-15a3de229384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.26\n",
      "Epoch 2/5 loss: 2.26\n",
      "Epoch 3/5 loss: 2.26\n",
      "Epoch 4/5 loss: 2.24\n",
      "Epoch 5/5 loss: 2.21\n"
     ]
    }
   ],
   "source": [
    "model_attention_4=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=4, out_dimension=10,patch_size=2)\n",
    "train(train_data_loader,model=model_attention_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6967c218-13ec-4ed7-92f0-f0e498b5c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.19\n",
      "Test accuracy: 25.10%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_attention_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792c8e2-cda1-401a-95d3-a80de69807ac",
   "metadata": {},
   "source": [
    "## Training with 8 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "931a90ae-042d-485f-9842-b365030f3b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 2.25\n",
      "Epoch 2/5 loss: 2.21\n",
      "Epoch 3/5 loss: 2.19\n",
      "Epoch 4/5 loss: 2.17\n",
      "Epoch 5/5 loss: 2.16\n"
     ]
    }
   ],
   "source": [
    "model_attention_8=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=2, hidden_dimension=8, num_heads=8, out_dimension=10,patch_size=2)\n",
    "train(train_data_loader,model=model_attention_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c5b41f-ff0f-4510-98ab-503c0c088fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.15\n",
      "Test accuracy: 30.02%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_attention_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda15ec4-8424-4761-8413-3f3f9b6ee717",
   "metadata": {},
   "source": [
    "## Training with 12 attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62043f48-655e-4b20-9eef-5874a3649e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attention_12=VisionTransformer(image_size=(BATCH_SIZE,3, 32, 32), num_encoder_blocks=5, hidden_dimension=12, num_heads=12, out_dimension=10,patch_size=2)\n",
    "train(train_data_loader,model=model_attention_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf0276d5-b5df-4169-85c7-f1bd82971925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.15\n",
      "Test accuracy: 30.07%\n"
     ]
    }
   ],
   "source": [
    "test(model=model_attention_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775cc92-985b-439b-934b-40d822f5460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visualize_attention_maps(model, test_dataset, num_classes):\n",
    "    model.eval()\n",
    "    for cls in range(num_classes):\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] == cls:\n",
    "                        outputs, attn_weights = model(images[i].unsqueeze(0))  # Add batch dimension\n",
    "                        # print(attn_weights)\n",
    "                        # Visualize attention maps\n",
    "                        for layer, attn_weight in enumerate(attn_weights):\n",
    "                            plt.imshow(attn_weight[layer])\n",
    "                            break\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "                #             attn_weight = attn_weight.squeeze(0)  # Remove batch dimension\n",
    "                #             num_heads = attn_weight.size(0)\n",
    "                #             for head in range(num_heads):\n",
    "                #                 plt.figure(figsize=(5, 5))\n",
    "                #                 plt.imshow(attn_weight[layer][head].cpu(), cmap='hot', interpolation='nearest')\n",
    "                #                 plt.title(f'Layer {layer + 1}, Head {head + 1}')\n",
    "                #                 plt.colorbar()\n",
    "                #                 plt.show()\n",
    "                #         count += 1\n",
    "                #         if count == 2:\n",
    "                #             break\n",
    "                # if count == 2:\n",
    "                #     break\n",
    "\n",
    "\n",
    "\n",
    "visualize_attention_maps(transformer_model, test_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae9a3a-f116-40b5-a400-ef30e4d44fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
